{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "try:\n",
    "    import xml.etree.cElementTree as ET\n",
    "except ImportError:\n",
    "    import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "import util\n",
    "\n",
    "\n",
    "def extract_feats(ffs, direc=\"train\", global_feat_dict=None):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      ffs are a list of feature-functions.\n",
    "      direc is a directory containing xml files (expected to be train or test).\n",
    "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
    "      should only be provided when extracting features from test data, so that \n",
    "      the columns of the test matrix align correctly.\n",
    "\n",
    "    returns: \n",
    "      a sparse design matrix, a dict mapping features to column-numbers,\n",
    "      a vector of target classes, and a list of system-call-history ids in order \n",
    "      of their rows in the design matrix.\n",
    "      \n",
    "      Note: the vector of target classes returned will contain the true indices of the\n",
    "      target classes on the training data, but will contain only -1's on the test\n",
    "      data\n",
    "    \"\"\"\n",
    "    fds = [] # list of feature dicts\n",
    "    classes = []\n",
    "    ids = [] \n",
    "    for datafile in os.listdir(direc):\n",
    "        # extract id and true class (if available) from filename\n",
    "        id_str,clazz = datafile.split('.')[:2]\n",
    "        ids.append(id_str)\n",
    "        # add target class if this is training data\n",
    "        try:\n",
    "            classes.append(util.malware_classes.index(clazz))\n",
    "        except ValueError:\n",
    "            # we should only fail to find the label in our list of malware classes\n",
    "            # if this is test data, which always has an \"X\" label\n",
    "            assert clazz == \"X\"\n",
    "            classes.append(-1)\n",
    "        rowfd = {}\n",
    "        # parse file as an xml document\n",
    "        tree = ET.parse(os.path.join(direc,datafile))\n",
    "        # accumulate features\n",
    "        [rowfd.update(ff(tree)) for ff in ffs]\n",
    "        fds.append(rowfd)\n",
    "        \n",
    "    X,feat_dict = make_design_mat(fds,global_feat_dict)\n",
    "    return X, feat_dict, np.array(classes), ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for debugging purposes\n",
    "tree = ET.parse(os.path.join('train','00bee48acc9d1774e4edf96f9582fac06b2ec1f14.None.xml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_design_mat(fds, global_feat_dict=None):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      fds is a list of feature dicts (one for each row).\n",
    "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
    "      should only be provided when extracting features from test data, so that \n",
    "      the columns of the test matrix align correctly.\n",
    "       \n",
    "    returns: \n",
    "        a sparse NxD design matrix, where N == len(fds) and D is the number of\n",
    "        the union of features defined in any of the fds \n",
    "    \"\"\"\n",
    "    if global_feat_dict is None:\n",
    "        all_feats = set()\n",
    "        [all_feats.update(fd.keys()) for fd in fds]\n",
    "        feat_dict = dict([(feat, i) for i, feat in enumerate(sorted(all_feats))])\n",
    "    else:\n",
    "        feat_dict = global_feat_dict\n",
    "        \n",
    "    cols = []\n",
    "    rows = []\n",
    "    data = []        \n",
    "    for i in xrange(len(fds)):\n",
    "        temp_cols = []\n",
    "        temp_data = []\n",
    "        for feat,val in fds[i].iteritems():\n",
    "            try:\n",
    "                # update temp_cols iff update temp_data\n",
    "                temp_cols.append(feat_dict[feat])\n",
    "                temp_data.append(val)\n",
    "            except KeyError as ex:\n",
    "                if global_feat_dict is not None:\n",
    "                    pass  # new feature in test data; nbd\n",
    "                else:\n",
    "                    raise ex\n",
    "\n",
    "        # all fd's features in the same row\n",
    "        k = len(temp_cols)\n",
    "        cols.extend(temp_cols)\n",
    "        data.extend(temp_data)\n",
    "        rows.extend([i]*k)\n",
    "\n",
    "    assert len(cols) == len(rows) and len(rows) == len(data)\n",
    "   \n",
    "\n",
    "    X = sparse.csr_matrix((np.array(data),\n",
    "                   (np.array(rows), np.array(cols))),\n",
    "                   shape=(len(fds), len(feat_dict)))\n",
    "    return X, feat_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Here are two example feature-functions. They each take an xml.etree.ElementTree object, \n",
    "# (i.e., the result of parsing an xml file) and returns a dictionary mapping \n",
    "# feature-names to numeric values.\n",
    "## TODO: modify these functions, and/or add new ones.\n",
    "def first_last_system_call_feats(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping 'first_call-x' to 1 if x was the first system call\n",
    "      made, and 'last_call-y' to 1 if y was the last system call made. \n",
    "      (in other words, it returns a dictionary indicating what the first and \n",
    "      last system calls made by an executable were.)\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    first = True # is this the first system call\n",
    "    last_call = None # keep track of last call we've seen\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            if first:\n",
    "                c[\"first_call-\"+el.tag] = 1\n",
    "                first = False\n",
    "            last_call = el.tag  # update last call seen\n",
    "            \n",
    "    # finally, mark last call seen\n",
    "    c[\"last_call-\"+last_call] = 1\n",
    "    return c\n",
    "\n",
    "def system_call_count_feats(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping 'num_system_calls' to the number of system_calls\n",
    "      made by an executable (summed over all processes)\n",
    "      as well as:\n",
    "         - number of each kind of tag\n",
    "         - proportion of each kind of tag\n",
    "         - average number of threads per process\n",
    "         - average number of systems calls per section\n",
    "         - average number of system calls per process\n",
    "         - total number of tags\n",
    "    \"\"\"\n",
    "    #initiate counters\n",
    "    c = Counter()\n",
    "    c_all = Counter()\n",
    "    n_el = 0\n",
    "    in_all_section = False\n",
    "    for el in tree.iter():\n",
    "        #keep track of all the kind of tags, and the total number of tags\n",
    "        c_all[\"num_\"+str(el.tag)] += 1\n",
    "        n_el += 1        \n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            c['num_system_calls'] += 1\n",
    "    #calculate proportions for each tag, and merge everything in a dictionary      \n",
    "    for key, val in c_all.items():\n",
    "        c[key] = val\n",
    "        c[\"ratio-\"+key] = float(val)/n_el\n",
    "    c[\"n_el\"] = n_el\n",
    "    if c['num_processes'] != 0:\n",
    "        c[\"ratio-threads-processes\"] = float(c['num_threads'])/c['num_processes']        \n",
    "    if c['num_sections'] != 0:\n",
    "        c[\"ratio-system_calls-sections\"] = float(c['num_system_calls'])/c['num_sections']\n",
    "    if c['num_processes'] != 0:\n",
    "        c[\"ratio-system_calls-processes\"] = float(c['num_system_calls'])/c['num_processes']\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.48107582\n",
      "Validation score: 0.533981\n",
      "Iteration 2, loss = 2.32736243\n",
      "Validation score: 0.533981\n",
      "Iteration 3, loss = 2.19125051\n",
      "Validation score: 0.533981\n",
      "Iteration 4, loss = 2.07697919\n",
      "Validation score: 0.533981\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 2.24406234\n",
      "Validation score: 0.533981\n",
      "Iteration 2, loss = 1.74400484\n",
      "Validation score: 0.533981\n",
      "Iteration 3, loss = 1.40400943\n",
      "Validation score: 0.527508\n",
      "Iteration 4, loss = 1.14270241\n",
      "Validation score: 0.624595\n",
      "Iteration 5, loss = 0.91486502\n",
      "Validation score: 0.792880\n",
      "Iteration 6, loss = 0.77249607\n",
      "Validation score: 0.796117\n",
      "Iteration 7, loss = 0.67192259\n",
      "Validation score: 0.799353\n",
      "Iteration 8, loss = 0.62762974\n",
      "Validation score: 0.812298\n",
      "Iteration 9, loss = 0.59437660\n",
      "Validation score: 0.844660\n",
      "Iteration 10, loss = 0.56524034\n",
      "Validation score: 0.844660\n",
      "Iteration 11, loss = 0.54533759\n",
      "Validation score: 0.851133\n",
      "Iteration 12, loss = 0.51828029\n",
      "Validation score: 0.841424\n",
      "Iteration 13, loss = 0.50674933\n",
      "Validation score: 0.860841\n",
      "Iteration 14, loss = 0.49864226\n",
      "Validation score: 0.857605\n",
      "Iteration 15, loss = 0.48919774\n",
      "Validation score: 0.860841\n",
      "Iteration 16, loss = 0.47294451\n",
      "Validation score: 0.857605\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.75434226\n",
      "Validation score: 0.689320\n",
      "Iteration 2, loss = 1.11661671\n",
      "Validation score: 0.695793\n",
      "Iteration 3, loss = 0.84123165\n",
      "Validation score: 0.776699\n",
      "Iteration 4, loss = 0.69141547\n",
      "Validation score: 0.799353\n",
      "Iteration 5, loss = 0.58835519\n",
      "Validation score: 0.802589\n",
      "Iteration 6, loss = 0.51824303\n",
      "Validation score: 0.815534\n",
      "Iteration 7, loss = 0.47044572\n",
      "Validation score: 0.822006\n",
      "Iteration 8, loss = 0.45118722\n",
      "Validation score: 0.825243\n",
      "Iteration 9, loss = 0.42293618\n",
      "Validation score: 0.828479\n",
      "Iteration 10, loss = 0.39916571\n",
      "Validation score: 0.828479\n",
      "Iteration 11, loss = 0.37665526\n",
      "Validation score: 0.828479\n",
      "Iteration 12, loss = 0.34938331\n",
      "Validation score: 0.831715\n",
      "Iteration 13, loss = 0.33124937\n",
      "Validation score: 0.838188\n",
      "Iteration 14, loss = 0.32573318\n",
      "Validation score: 0.854369\n",
      "Iteration 15, loss = 0.32126781\n",
      "Validation score: 0.851133\n",
      "Iteration 16, loss = 0.30699393\n",
      "Validation score: 0.854369\n",
      "Iteration 17, loss = 0.30049330\n",
      "Validation score: 0.854369\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 2.16227214\n",
      "Validation score: 0.686084\n",
      "Iteration 2, loss = 1.08492763\n",
      "Validation score: 0.715210\n",
      "Iteration 3, loss = 0.79113232\n",
      "Validation score: 0.796117\n",
      "Iteration 4, loss = 0.64791329\n",
      "Validation score: 0.802589\n",
      "Iteration 5, loss = 0.57073459\n",
      "Validation score: 0.828479\n",
      "Iteration 6, loss = 0.51180952\n",
      "Validation score: 0.854369\n",
      "Iteration 7, loss = 0.46457149\n",
      "Validation score: 0.851133\n",
      "Iteration 8, loss = 0.41748579\n",
      "Validation score: 0.873786\n",
      "Iteration 9, loss = 0.38922142\n",
      "Validation score: 0.867314\n",
      "Iteration 10, loss = 0.36884915\n",
      "Validation score: 0.847896\n",
      "Iteration 11, loss = 0.33704691\n",
      "Validation score: 0.857605\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.49509471\n",
      "Validation score: 0.760518\n",
      "Iteration 2, loss = 0.80346149\n",
      "Validation score: 0.792880\n",
      "Iteration 3, loss = 0.58308060\n",
      "Validation score: 0.828479\n",
      "Iteration 4, loss = 0.46869289\n",
      "Validation score: 0.831715\n",
      "Iteration 5, loss = 0.39288661\n",
      "Validation score: 0.841424\n",
      "Iteration 6, loss = 0.33918435\n",
      "Validation score: 0.847896\n",
      "Iteration 7, loss = 0.30598916\n",
      "Validation score: 0.847896\n",
      "Iteration 8, loss = 0.28002260\n",
      "Validation score: 0.860841\n",
      "Iteration 9, loss = 0.26727737\n",
      "Validation score: 0.860841\n",
      "Iteration 10, loss = 0.26219693\n",
      "Validation score: 0.851133\n",
      "Iteration 11, loss = 0.24066507\n",
      "Validation score: 0.857605\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.32797249\n",
      "Validation score: 0.792880\n",
      "Iteration 2, loss = 0.65862315\n",
      "Validation score: 0.812298\n",
      "Iteration 3, loss = 0.51219009\n",
      "Validation score: 0.834951\n",
      "Iteration 4, loss = 0.42486141\n",
      "Validation score: 0.851133\n",
      "Iteration 5, loss = 0.37015385\n",
      "Validation score: 0.857605\n",
      "Iteration 6, loss = 0.32449821\n",
      "Validation score: 0.847896\n",
      "Iteration 7, loss = 0.29334681\n",
      "Validation score: 0.867314\n",
      "Iteration 8, loss = 0.27071133\n",
      "Validation score: 0.860841\n",
      "Iteration 9, loss = 0.25588977\n",
      "Validation score: 0.870550\n",
      "Iteration 10, loss = 0.25207364\n",
      "Validation score: 0.870550\n",
      "Iteration 11, loss = 0.24129168\n",
      "Validation score: 0.873786\n",
      "Iteration 12, loss = 0.23337070\n",
      "Validation score: 0.870550\n",
      "Iteration 13, loss = 0.21987512\n",
      "Validation score: 0.870550\n",
      "Iteration 14, loss = 0.22598603\n",
      "Validation score: 0.867314\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.44643498\n",
      "Validation score: 0.786408\n",
      "Iteration 2, loss = 0.68852865\n",
      "Validation score: 0.815534\n",
      "Iteration 3, loss = 0.50326797\n",
      "Validation score: 0.844660\n",
      "Iteration 4, loss = 0.40373014\n",
      "Validation score: 0.864078\n",
      "Iteration 5, loss = 0.35118267\n",
      "Validation score: 0.851133\n",
      "Iteration 6, loss = 0.31525544\n",
      "Validation score: 0.867314\n",
      "Iteration 7, loss = 0.28849830\n",
      "Validation score: 0.860841\n",
      "Iteration 8, loss = 0.28173903\n",
      "Validation score: 0.860841\n",
      "Iteration 9, loss = 0.24838646\n",
      "Validation score: 0.867314\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.33519505\n",
      "Validation score: 0.799353\n",
      "Iteration 2, loss = 0.65507281\n",
      "Validation score: 0.851133\n",
      "Iteration 3, loss = 0.50752828\n",
      "Validation score: 0.864078\n",
      "Iteration 4, loss = 0.41734477\n",
      "Validation score: 0.873786\n",
      "Iteration 5, loss = 0.36013138\n",
      "Validation score: 0.880259\n",
      "Iteration 6, loss = 0.32215157\n",
      "Validation score: 0.889968\n",
      "Iteration 7, loss = 0.29824114\n",
      "Validation score: 0.864078\n",
      "Iteration 8, loss = 0.25867639\n",
      "Validation score: 0.870550\n",
      "Iteration 9, loss = 0.24016989\n",
      "Validation score: 0.880259\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.17927947\n",
      "Validation score: 0.815534\n",
      "Iteration 2, loss = 0.58281996\n",
      "Validation score: 0.880259\n",
      "Iteration 3, loss = 0.44333917\n",
      "Validation score: 0.864078\n",
      "Iteration 4, loss = 0.36678586\n",
      "Validation score: 0.883495\n",
      "Iteration 5, loss = 0.31493295\n",
      "Validation score: 0.906149\n",
      "Iteration 6, loss = 0.29506121\n",
      "Validation score: 0.889968\n",
      "Iteration 7, loss = 0.28360760\n",
      "Validation score: 0.906149\n",
      "Iteration 8, loss = 0.26488506\n",
      "Validation score: 0.880259\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.27870874\n",
      "Validation score: 0.776699\n",
      "Iteration 2, loss = 0.61255551\n",
      "Validation score: 0.805825\n",
      "Iteration 3, loss = 0.44833153\n",
      "Validation score: 0.870550\n",
      "Iteration 4, loss = 0.35740307\n",
      "Validation score: 0.877023\n",
      "Iteration 5, loss = 0.31063244\n",
      "Validation score: 0.886731\n",
      "Iteration 6, loss = 0.28695149\n",
      "Validation score: 0.880259\n",
      "Iteration 7, loss = 0.24926662\n",
      "Validation score: 0.877023\n",
      "Iteration 8, loss = 0.23561939\n",
      "Validation score: 0.886731\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.23886621\n",
      "Validation score: 0.825243\n",
      "Iteration 2, loss = 0.58644910\n",
      "Validation score: 0.867314\n",
      "Iteration 3, loss = 0.43259742\n",
      "Validation score: 0.886731\n",
      "Iteration 4, loss = 0.35363144\n",
      "Validation score: 0.889968\n",
      "Iteration 5, loss = 0.31194284\n",
      "Validation score: 0.883495\n",
      "Iteration 6, loss = 0.26798972\n",
      "Validation score: 0.889968\n",
      "Iteration 7, loss = 0.25211773\n",
      "Validation score: 0.889968\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.07738387\n",
      "Validation score: 0.812298\n",
      "Iteration 2, loss = 0.52029519\n",
      "Validation score: 0.870550\n",
      "Iteration 3, loss = 0.39319350\n",
      "Validation score: 0.873786\n",
      "Iteration 4, loss = 0.33502195\n",
      "Validation score: 0.847896\n",
      "Iteration 5, loss = 0.28974275\n",
      "Validation score: 0.877023\n",
      "Iteration 6, loss = 0.25218274\n",
      "Validation score: 0.857605\n",
      "Iteration 7, loss = 0.23118197\n",
      "Validation score: 0.834951\n",
      "Iteration 8, loss = 0.24035004\n",
      "Validation score: 0.889968\n",
      "Iteration 9, loss = 0.24260925\n",
      "Validation score: 0.867314\n",
      "Iteration 10, loss = 0.21841537\n",
      "Validation score: 0.873786\n",
      "Iteration 11, loss = 0.19189267\n",
      "Validation score: 0.883495\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.19775526\n",
      "Validation score: 0.773463\n",
      "Iteration 2, loss = 0.55191032\n",
      "Validation score: 0.841424\n",
      "Iteration 3, loss = 0.42646032\n",
      "Validation score: 0.877023\n",
      "Iteration 4, loss = 0.35475465\n",
      "Validation score: 0.886731\n",
      "Iteration 5, loss = 0.30055673\n",
      "Validation score: 0.889968\n",
      "Iteration 6, loss = 0.25989763\n",
      "Validation score: 0.886731\n",
      "Iteration 7, loss = 0.24344441\n",
      "Validation score: 0.873786\n",
      "Iteration 8, loss = 0.23491516\n",
      "Validation score: 0.873786\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.27511555\n",
      "Validation score: 0.786408\n",
      "Iteration 2, loss = 0.57308250\n",
      "Validation score: 0.834951\n",
      "Iteration 3, loss = 0.40976114\n",
      "Validation score: 0.834951\n",
      "Iteration 4, loss = 0.33404466\n",
      "Validation score: 0.844660\n",
      "Iteration 5, loss = 0.30195536\n",
      "Validation score: 0.854369\n",
      "Iteration 6, loss = 0.28034382\n",
      "Validation score: 0.860841\n",
      "Iteration 7, loss = 0.25231742\n",
      "Validation score: 0.860841\n",
      "Iteration 8, loss = 0.22839691\n",
      "Validation score: 0.857605\n",
      "Iteration 9, loss = 0.21436351\n",
      "Validation score: 0.841424\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.10457798\n",
      "Validation score: 0.828479\n",
      "Iteration 2, loss = 0.51804731\n",
      "Validation score: 0.860841\n",
      "Iteration 3, loss = 0.40728123\n",
      "Validation score: 0.867314\n",
      "Iteration 4, loss = 0.33287025\n",
      "Validation score: 0.867314\n",
      "Iteration 5, loss = 0.29428769\n",
      "Validation score: 0.877023\n",
      "Iteration 6, loss = 0.26060265\n",
      "Validation score: 0.877023\n",
      "Iteration 7, loss = 0.25053924\n",
      "Validation score: 0.873786\n",
      "Iteration 8, loss = 0.21929447\n",
      "Validation score: 0.860841\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.19366837\n",
      "Validation score: 0.822006\n",
      "Iteration 2, loss = 0.55846877\n",
      "Validation score: 0.834951\n",
      "Iteration 3, loss = 0.42201401\n",
      "Validation score: 0.860841\n",
      "Iteration 4, loss = 0.33657163\n",
      "Validation score: 0.847896\n",
      "Iteration 5, loss = 0.29624247\n",
      "Validation score: 0.870550\n",
      "Iteration 6, loss = 0.25500794\n",
      "Validation score: 0.870550\n",
      "Iteration 7, loss = 0.24768459\n",
      "Validation score: 0.873786\n",
      "Iteration 8, loss = 0.22774355\n",
      "Validation score: 0.880259\n",
      "Iteration 9, loss = 0.20781736\n",
      "Validation score: 0.860841\n",
      "Iteration 10, loss = 0.18922045\n",
      "Validation score: 0.877023\n",
      "Iteration 11, loss = 0.18276721\n",
      "Validation score: 0.880259\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.21419640\n",
      "Validation score: 0.809061\n",
      "Iteration 2, loss = 0.54017212\n",
      "Validation score: 0.847896\n",
      "Iteration 3, loss = 0.40775091\n",
      "Validation score: 0.838188\n",
      "Iteration 4, loss = 0.35063142\n",
      "Validation score: 0.860841\n",
      "Iteration 5, loss = 0.30505251\n",
      "Validation score: 0.877023\n",
      "Iteration 6, loss = 0.26329402\n",
      "Validation score: 0.870550\n",
      "Iteration 7, loss = 0.25240583\n",
      "Validation score: 0.893204\n",
      "Iteration 8, loss = 0.22997611\n",
      "Validation score: 0.883495\n",
      "Iteration 9, loss = 0.20103092\n",
      "Validation score: 0.873786\n",
      "Iteration 10, loss = 0.19738297\n",
      "Validation score: 0.883495\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.04293220\n",
      "Validation score: 0.812298\n",
      "Iteration 2, loss = 0.49407223\n",
      "Validation score: 0.844660\n",
      "Iteration 3, loss = 0.37482451\n",
      "Validation score: 0.847896\n",
      "Iteration 4, loss = 0.33233460\n",
      "Validation score: 0.864078\n",
      "Iteration 5, loss = 0.27873693\n",
      "Validation score: 0.857605\n",
      "Iteration 6, loss = 0.24631066\n",
      "Validation score: 0.844660\n",
      "Iteration 7, loss = 0.23864430\n",
      "Validation score: 0.844660\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.19409949\n",
      "Validation score: 0.802589\n",
      "Iteration 2, loss = 0.53793325\n",
      "Validation score: 0.864078\n",
      "Iteration 3, loss = 0.41607851\n",
      "Validation score: 0.870550\n",
      "Iteration 4, loss = 0.32946542\n",
      "Validation score: 0.857605\n",
      "Iteration 5, loss = 0.29827272\n",
      "Validation score: 0.893204\n",
      "Iteration 6, loss = 0.26057497\n",
      "Validation score: 0.873786\n",
      "Iteration 7, loss = 0.25126886\n",
      "Validation score: 0.886731\n",
      "Iteration 8, loss = 0.21831313\n",
      "Validation score: 0.867314\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.11753148\n",
      "Validation score: 0.825243\n",
      "Iteration 2, loss = 0.50886733\n",
      "Validation score: 0.844660\n",
      "Iteration 3, loss = 0.39047975\n",
      "Validation score: 0.860841\n",
      "Iteration 4, loss = 0.31694150\n",
      "Validation score: 0.860841\n",
      "Iteration 5, loss = 0.27243028\n",
      "Validation score: 0.864078\n",
      "Iteration 6, loss = 0.26229523\n",
      "Validation score: 0.877023\n",
      "Iteration 7, loss = 0.23794136\n",
      "Validation score: 0.860841\n",
      "Iteration 8, loss = 0.21353792\n",
      "Validation score: 0.844660\n",
      "Iteration 9, loss = 0.21673495\n",
      "Validation score: 0.864078\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.09256256\n",
      "Validation score: 0.815534\n",
      "Iteration 2, loss = 0.50986870\n",
      "Validation score: 0.867314\n",
      "Iteration 3, loss = 0.38538647\n",
      "Validation score: 0.873786\n",
      "Iteration 4, loss = 0.30575440\n",
      "Validation score: 0.883495\n",
      "Iteration 5, loss = 0.27738857\n",
      "Validation score: 0.873786\n",
      "Iteration 6, loss = 0.25709880\n",
      "Validation score: 0.880259\n",
      "Iteration 7, loss = 0.23044319\n",
      "Validation score: 0.877023\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.14494447\n",
      "Validation score: 0.834951\n",
      "Iteration 2, loss = 0.48409095\n",
      "Validation score: 0.867314\n",
      "Iteration 3, loss = 0.38131972\n",
      "Validation score: 0.864078\n",
      "Iteration 4, loss = 0.32431136\n",
      "Validation score: 0.877023\n",
      "Iteration 5, loss = 0.29356797\n",
      "Validation score: 0.870550\n",
      "Iteration 6, loss = 0.26159090\n",
      "Validation score: 0.867314\n",
      "Iteration 7, loss = 0.24935266\n",
      "Validation score: 0.867314\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.06395225\n",
      "Validation score: 0.799353\n",
      "Iteration 2, loss = 0.49158609\n",
      "Validation score: 0.838188\n",
      "Iteration 3, loss = 0.38088889\n",
      "Validation score: 0.851133\n",
      "Iteration 4, loss = 0.31141002\n",
      "Validation score: 0.851133\n",
      "Iteration 5, loss = 0.29392009\n",
      "Validation score: 0.857605\n",
      "Iteration 6, loss = 0.24791341\n",
      "Validation score: 0.851133\n",
      "Iteration 7, loss = 0.23024366\n",
      "Validation score: 0.847896\n",
      "Iteration 8, loss = 0.22041886\n",
      "Validation score: 0.847896\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.11175677\n",
      "Validation score: 0.854369\n",
      "Iteration 2, loss = 0.49885524\n",
      "Validation score: 0.886731\n",
      "Iteration 3, loss = 0.40810271\n",
      "Validation score: 0.909385\n",
      "Iteration 4, loss = 0.33594155\n",
      "Validation score: 0.919094\n",
      "Iteration 5, loss = 0.29201810\n",
      "Validation score: 0.909385\n",
      "Iteration 6, loss = 0.25696820\n",
      "Validation score: 0.919094\n",
      "Iteration 7, loss = 0.24464409\n",
      "Validation score: 0.915858\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.05886996\n",
      "Validation score: 0.841424\n",
      "Iteration 2, loss = 0.47320901\n",
      "Validation score: 0.867314\n",
      "Iteration 3, loss = 0.38182973\n",
      "Validation score: 0.860841\n",
      "Iteration 4, loss = 0.31643577\n",
      "Validation score: 0.867314\n",
      "Iteration 5, loss = 0.27572715\n",
      "Validation score: 0.867314\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.10874357\n",
      "Validation score: 0.799353\n",
      "Iteration 2, loss = 0.48318268\n",
      "Validation score: 0.825243\n",
      "Iteration 3, loss = 0.37968333\n",
      "Validation score: 0.834951\n",
      "Iteration 4, loss = 0.31708238\n",
      "Validation score: 0.847896\n",
      "Iteration 5, loss = 0.27837408\n",
      "Validation score: 0.831715\n",
      "Iteration 6, loss = 0.24892703\n",
      "Validation score: 0.828479\n",
      "Iteration 7, loss = 0.21889381\n",
      "Validation score: 0.834951\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.10266164\n",
      "Validation score: 0.825243\n",
      "Iteration 2, loss = 0.49000183\n",
      "Validation score: 0.864078\n",
      "Iteration 3, loss = 0.37545621\n",
      "Validation score: 0.864078\n",
      "Iteration 4, loss = 0.32552016\n",
      "Validation score: 0.867314\n",
      "Iteration 5, loss = 0.27646969\n",
      "Validation score: 0.860841\n",
      "Iteration 6, loss = 0.24515090\n",
      "Validation score: 0.870550\n",
      "Iteration 7, loss = 0.21286507\n",
      "Validation score: 0.860841\n",
      "Iteration 8, loss = 0.20064090\n",
      "Validation score: 0.847896\n",
      "Iteration 9, loss = 0.21167220\n",
      "Validation score: 0.867314\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.18225830\n",
      "Validation score: 0.851133\n",
      "Iteration 2, loss = 0.49338039\n",
      "Validation score: 0.870550\n",
      "Iteration 3, loss = 0.38885552\n",
      "Validation score: 0.877023\n",
      "Iteration 4, loss = 0.32833071\n",
      "Validation score: 0.870550\n",
      "Iteration 5, loss = 0.29175984\n",
      "Validation score: 0.860841\n",
      "Iteration 6, loss = 0.26076559\n",
      "Validation score: 0.867314\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.15431335\n",
      "Validation score: 0.867314\n",
      "Iteration 2, loss = 0.54235816\n",
      "Validation score: 0.880259\n",
      "Iteration 3, loss = 0.41884874\n",
      "Validation score: 0.896440\n",
      "Iteration 4, loss = 0.33616260\n",
      "Validation score: 0.893204\n",
      "Iteration 5, loss = 0.29032485\n",
      "Validation score: 0.880259\n",
      "Iteration 6, loss = 0.28606639\n",
      "Validation score: 0.906149\n",
      "Iteration 7, loss = 0.26319773\n",
      "Validation score: 0.889968\n",
      "Iteration 8, loss = 0.23932798\n",
      "Validation score: 0.889968\n",
      "Iteration 9, loss = 0.21073534\n",
      "Validation score: 0.886731\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.13856327\n",
      "Validation score: 0.825243\n",
      "Iteration 2, loss = 0.49143849\n",
      "Validation score: 0.838188\n",
      "Iteration 3, loss = 0.38898697\n",
      "Validation score: 0.854369\n",
      "Iteration 4, loss = 0.33456461\n",
      "Validation score: 0.864078\n",
      "Iteration 5, loss = 0.26938821\n",
      "Validation score: 0.857605\n",
      "Iteration 6, loss = 0.26683850\n",
      "Validation score: 0.870550\n",
      "Iteration 7, loss = 0.24048416\n",
      "Validation score: 0.867314\n",
      "Iteration 8, loss = 0.22739515\n",
      "Validation score: 0.854369\n",
      "Iteration 9, loss = 0.21946937\n",
      "Validation score: 0.857605\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.18760417\n",
      "Validation score: 0.854369\n",
      "Iteration 2, loss = 0.52557791\n",
      "Validation score: 0.880259\n",
      "Iteration 3, loss = 0.39474367\n",
      "Validation score: 0.899676\n",
      "Iteration 4, loss = 0.32366019\n",
      "Validation score: 0.899676\n",
      "Iteration 5, loss = 0.28604995\n",
      "Validation score: 0.896440\n",
      "Iteration 6, loss = 0.28492030\n",
      "Validation score: 0.893204\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.18800028\n",
      "Validation score: 0.779935\n",
      "Iteration 2, loss = 0.53160611\n",
      "Validation score: 0.841424\n",
      "Iteration 3, loss = 0.38065771\n",
      "Validation score: 0.867314\n",
      "Iteration 4, loss = 0.31809156\n",
      "Validation score: 0.877023\n",
      "Iteration 5, loss = 0.30462436\n",
      "Validation score: 0.883495\n",
      "Iteration 6, loss = 0.27147875\n",
      "Validation score: 0.873786\n",
      "Iteration 7, loss = 0.23903953\n",
      "Validation score: 0.877023\n",
      "Iteration 8, loss = 0.22191738\n",
      "Validation score: 0.886731\n",
      "Iteration 9, loss = 0.21789701\n",
      "Validation score: 0.880259\n",
      "Iteration 10, loss = 0.20428505\n",
      "Validation score: 0.873786\n",
      "Iteration 11, loss = 0.20159915\n",
      "Validation score: 0.889968\n",
      "Iteration 12, loss = 0.19392887\n",
      "Validation score: 0.896440\n",
      "Iteration 13, loss = 0.18847454\n",
      "Validation score: 0.883495\n",
      "Iteration 14, loss = 0.17113442\n",
      "Validation score: 0.889968\n",
      "Iteration 15, loss = 0.16737513\n",
      "Validation score: 0.889968\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.13503263\n",
      "Validation score: 0.851133\n",
      "Iteration 2, loss = 0.49920387\n",
      "Validation score: 0.857605\n",
      "Iteration 3, loss = 0.36783428\n",
      "Validation score: 0.870550\n",
      "Iteration 4, loss = 0.30649319\n",
      "Validation score: 0.877023\n",
      "Iteration 5, loss = 0.27874549\n",
      "Validation score: 0.889968\n",
      "Iteration 6, loss = 0.24543923\n",
      "Validation score: 0.886731\n",
      "Iteration 7, loss = 0.22002475\n",
      "Validation score: 0.867314\n",
      "Iteration 8, loss = 0.20491013\n",
      "Validation score: 0.877023\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.27873324\n",
      "Validation score: 0.825243\n",
      "Iteration 2, loss = 0.50874707\n",
      "Validation score: 0.851133\n",
      "Iteration 3, loss = 0.40543813\n",
      "Validation score: 0.864078\n",
      "Iteration 4, loss = 0.34419316\n",
      "Validation score: 0.867314\n",
      "Iteration 5, loss = 0.27778760\n",
      "Validation score: 0.870550\n",
      "Iteration 6, loss = 0.25379820\n",
      "Validation score: 0.880259\n",
      "Iteration 7, loss = 0.23136568\n",
      "Validation score: 0.877023\n",
      "Iteration 8, loss = 0.23457208\n",
      "Validation score: 0.883495\n",
      "Iteration 9, loss = 0.21664189\n",
      "Validation score: 0.902913\n",
      "Iteration 10, loss = 0.19317474\n",
      "Validation score: 0.896440\n",
      "Iteration 11, loss = 0.18137705\n",
      "Validation score: 0.880259\n",
      "Iteration 12, loss = 0.18588172\n",
      "Validation score: 0.877023\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.21793074\n",
      "Validation score: 0.792880\n",
      "Iteration 2, loss = 0.53407576\n",
      "Validation score: 0.867314\n",
      "Iteration 3, loss = 0.39754286\n",
      "Validation score: 0.883495\n",
      "Iteration 4, loss = 0.33275362\n",
      "Validation score: 0.896440\n",
      "Iteration 5, loss = 0.29336452\n",
      "Validation score: 0.883495\n",
      "Iteration 6, loss = 0.27144439\n",
      "Validation score: 0.880259\n",
      "Iteration 7, loss = 0.23592568\n",
      "Validation score: 0.877023\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.33499950\n",
      "Validation score: 0.831715\n",
      "Iteration 2, loss = 0.52177645\n",
      "Validation score: 0.860841\n",
      "Iteration 3, loss = 0.40273668\n",
      "Validation score: 0.870550\n",
      "Iteration 4, loss = 0.33373738\n",
      "Validation score: 0.854369\n",
      "Iteration 5, loss = 0.27564092\n",
      "Validation score: 0.867314\n",
      "Iteration 6, loss = 0.25223313\n",
      "Validation score: 0.870550\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.08528583\n",
      "Validation score: 0.838188\n",
      "Iteration 2, loss = 0.48473663\n",
      "Validation score: 0.854369\n",
      "Iteration 3, loss = 0.38944861\n",
      "Validation score: 0.838188\n",
      "Iteration 4, loss = 0.31846985\n",
      "Validation score: 0.854369\n",
      "Iteration 5, loss = 0.28866837\n",
      "Validation score: 0.847896\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.32542151\n",
      "Validation score: 0.838188\n",
      "Iteration 2, loss = 0.55672504\n",
      "Validation score: 0.870550\n",
      "Iteration 3, loss = 0.41657988\n",
      "Validation score: 0.877023\n",
      "Iteration 4, loss = 0.35452233\n",
      "Validation score: 0.889968\n",
      "Iteration 5, loss = 0.30262247\n",
      "Validation score: 0.883495\n",
      "Iteration 6, loss = 0.25618679\n",
      "Validation score: 0.873786\n",
      "Iteration 7, loss = 0.24539338\n",
      "Validation score: 0.886731\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.41209246\n",
      "Validation score: 0.792880\n",
      "Iteration 2, loss = 0.54536104\n",
      "Validation score: 0.825243\n",
      "Iteration 3, loss = 0.41245404\n",
      "Validation score: 0.854369\n",
      "Iteration 4, loss = 0.35360374\n",
      "Validation score: 0.854369\n",
      "Iteration 5, loss = 0.32088639\n",
      "Validation score: 0.873786\n",
      "Iteration 6, loss = 0.27677105\n",
      "Validation score: 0.873786\n",
      "Iteration 7, loss = 0.23876364\n",
      "Validation score: 0.880259\n",
      "Iteration 8, loss = 0.20625939\n",
      "Validation score: 0.873786\n",
      "Iteration 9, loss = 0.20104854\n",
      "Validation score: 0.873786\n",
      "Iteration 10, loss = 0.18222239\n",
      "Validation score: 0.886731\n",
      "Iteration 11, loss = 0.17998345\n",
      "Validation score: 0.880259\n",
      "Iteration 12, loss = 0.17954714\n",
      "Validation score: 0.880259\n",
      "Iteration 13, loss = 0.16740735\n",
      "Validation score: 0.886731\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.33831460\n",
      "Validation score: 0.825243\n",
      "Iteration 2, loss = 0.57467835\n",
      "Validation score: 0.867314\n",
      "Iteration 3, loss = 0.43782349\n",
      "Validation score: 0.877023\n",
      "Iteration 4, loss = 0.36101212\n",
      "Validation score: 0.877023\n",
      "Iteration 5, loss = 0.31526939\n",
      "Validation score: 0.877023\n",
      "Iteration 6, loss = 0.26690998\n",
      "Validation score: 0.886731\n",
      "Iteration 7, loss = 0.25190574\n",
      "Validation score: 0.877023\n",
      "Iteration 8, loss = 0.22352312\n",
      "Validation score: 0.877023\n",
      "Iteration 9, loss = 0.20367944\n",
      "Validation score: 0.886731\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n",
      "Iteration 1, loss = 1.24044532\n",
      "Validation score: 0.779935\n",
      "Iteration 2, loss = 0.52578951\n",
      "Validation score: 0.838188\n",
      "Iteration 3, loss = 0.43122664\n",
      "Validation score: 0.838188\n",
      "Iteration 4, loss = 0.34776255\n",
      "Validation score: 0.854369\n",
      "Iteration 5, loss = 0.31330578\n",
      "Validation score: 0.864078\n",
      "Iteration 6, loss = 0.26520801\n",
      "Validation score: 0.867314\n",
      "Iteration 7, loss = 0.22593009\n",
      "Validation score: 0.870550\n",
      "Iteration 8, loss = 0.22183009\n",
      "Validation score: 0.864078\n",
      "Iteration 9, loss = 0.19848605\n",
      "Validation score: 0.873786\n",
      "Iteration 10, loss = 0.17824976\n",
      "Validation score: 0.877023\n",
      "Iteration 11, loss = 0.19921588\n",
      "Validation score: 0.857605\n",
      "Iteration 12, loss = 0.19177786\n",
      "Validation score: 0.870550\n",
      "Iteration 13, loss = 0.17069340\n",
      "Validation score: 0.870550\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "done learning\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x14f969b0>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def main():\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# extract features from smaller dataset\n",
    "\n",
    "train_dir = \"train_small\"\n",
    "test_dir = \"test_small\"\n",
    "outputfile = \"mypredictions.csv\"  # feel free to change this or take it as an argument\n",
    "\n",
    "# TODO put the names of the feature functions you've defined above in this list\n",
    "ffs = [first_last_system_call_feats, system_call_count_feats]\n",
    "\n",
    "\n",
    "print \"extracting training features...\"\n",
    "X_train,global_feat_dict,t_train,train_ids = extract_feats(ffs, train_dir)\n",
    "print \"done extracting training features\"\n",
    "print\n",
    "\n",
    "\n",
    "print \"extracting test features...\"\n",
    "X_test,_,t_ignore,test_ids = extract_feats(ffs, test_dir, global_feat_dict=global_feat_dict)\n",
    "print \"done extracting test features\"\n",
    "print\n",
    "\n",
    "#MLP parameters\n",
    "solver = 'adam'\n",
    "max_iter = 100\n",
    "\n",
    "\n",
    "alpha = 1e-6\n",
    "tol = 1e-12\n",
    "rate_init = 0.01\n",
    "learning_rate = 'adaptive'\n",
    "early_stopping = True\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "#Vary number of Nodes \n",
    "N_nodes = np.linspace(1, 201, 41)\n",
    "N_layers = 3\n",
    "\n",
    "for nodes in N_nodes:\n",
    "\n",
    "    layers = np.multiply(nodes, np.ones(N_layers)) \n",
    "    scaler = StandardScaler(with_mean = False)\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=layers, max_iter=max_iter, alpha=alpha,\n",
    "                        solver=solver, verbose=10, tol=tol, random_state=1,\n",
    "                        learning_rate_init=rate_init, learning_rate = learning_rate,\n",
    "                       early_stopping = early_stopping)\n",
    "    mlp.fit(X_train_scaled, t_train)\n",
    "    print \"done learning\"\n",
    "    print\n",
    "\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    preds = mlp.predict(X_test_scaled)\n",
    "    #print \"done making predictions\"\n",
    "    #print\n",
    "\n",
    "    #print \"writing predictions...\"\n",
    "    #util.write_predictions(preds, test_ids, outputfile)\n",
    "    #print \"done!\"\n",
    "\n",
    "    #test predictions\n",
    "    num_correct = np.sum(t_ignore == preds);\n",
    "    accuracy = np.append(accuracy,num_correct/float(t_ignore.size))\n",
    "    #print('Accuracy  = '+str(accuracy))\n",
    "plt.clf()\n",
    "plt.plot(N_nodes, accuracy, linestyle='--', marker='o', color='b')\n",
    "#plt.title('N_nodes = '+ str(N_nodes)+'_N_layers = ' + str(N_layers))\n",
    "plt.xlabel('Number of nodes')\n",
    "plt.ylabel('Accuracy')\n",
    "#plt.xlim((n_vals[0]-1, n_vals[-1]+1))\n",
    "#plt.ylim((min(RMSE), max(RMSE)))\n",
    "#plt.show()\n",
    "plt.savefig('Plots/'+_N_nodes_'+ str(N_nodes[[1]])+'-'+str(N_nodes[[-1]])+'_N_layers' + str(N_layers)+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.18325777\n",
      "Iteration 2, loss = 0.91712465\n",
      "Iteration 3, loss = 0.61017820\n",
      "Iteration 4, loss = 0.44364811\n",
      "Iteration 5, loss = 0.35111230\n",
      "Iteration 6, loss = 0.29204699\n",
      "Iteration 7, loss = 0.25798270\n",
      "Iteration 8, loss = 0.22419806\n",
      "Iteration 9, loss = 0.20543412\n",
      "Iteration 10, loss = 0.19113647\n",
      "Iteration 11, loss = 0.17874617\n",
      "Iteration 12, loss = 0.15886617\n",
      "Iteration 13, loss = 0.14713972\n",
      "Iteration 14, loss = 0.13732509\n",
      "Iteration 15, loss = 0.12546136\n",
      "Iteration 16, loss = 0.11614377\n",
      "Iteration 17, loss = 0.10977312\n",
      "Iteration 18, loss = 0.10478274\n",
      "Iteration 19, loss = 0.10186774\n",
      "Iteration 20, loss = 0.09619029\n",
      "Iteration 21, loss = 0.09096884\n",
      "Iteration 22, loss = 0.08841356\n",
      "Iteration 23, loss = 0.08755624\n",
      "Iteration 24, loss = 0.08324524\n",
      "Iteration 25, loss = 0.07908845\n",
      "Iteration 26, loss = 0.08118494\n",
      "Iteration 27, loss = 0.07662288\n",
      "Iteration 28, loss = 0.07310190\n",
      "Iteration 29, loss = 0.07082420\n",
      "Iteration 30, loss = 0.07041504\n",
      "Iteration 31, loss = 0.07278512\n",
      "Iteration 32, loss = 0.06946373\n",
      "Iteration 33, loss = 0.07066137\n",
      "Iteration 34, loss = 0.06599393\n",
      "Iteration 35, loss = 0.06624048\n",
      "Iteration 36, loss = 0.06397755\n",
      "Iteration 37, loss = 0.06601500\n",
      "Iteration 38, loss = 0.06944378\n",
      "Iteration 39, loss = 0.06547327\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.31626519\n",
      "Iteration 2, loss = 1.03616021\n",
      "Iteration 3, loss = 0.73113318\n",
      "Iteration 4, loss = 0.54530103\n",
      "Iteration 5, loss = 0.43684917\n",
      "Iteration 6, loss = 0.37967820\n",
      "Iteration 7, loss = 0.32028843\n",
      "Iteration 8, loss = 0.27818939\n",
      "Iteration 9, loss = 0.23970220\n",
      "Iteration 10, loss = 0.21335301\n",
      "Iteration 11, loss = 0.18428601\n",
      "Iteration 12, loss = 0.16439943\n",
      "Iteration 13, loss = 0.15201303\n",
      "Iteration 14, loss = 0.13187100\n",
      "Iteration 15, loss = 0.12011540\n",
      "Iteration 16, loss = 0.11527907\n",
      "Iteration 17, loss = 0.10645645\n",
      "Iteration 18, loss = 0.10388071\n",
      "Iteration 19, loss = 0.10272139\n",
      "Iteration 20, loss = 0.09056150\n",
      "Iteration 21, loss = 0.08925596\n",
      "Iteration 22, loss = 0.08244718\n",
      "Iteration 23, loss = 0.08331808\n",
      "Iteration 24, loss = 0.07219478\n",
      "Iteration 25, loss = 0.07732535\n",
      "Iteration 26, loss = 0.08331565\n",
      "Iteration 27, loss = 0.07005912\n",
      "Iteration 28, loss = 0.08868297\n",
      "Iteration 29, loss = 0.07241654\n",
      "Iteration 30, loss = 0.07100173\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.69742053\n",
      "Iteration 2, loss = 1.45256047\n",
      "Iteration 3, loss = 0.96347291\n",
      "Iteration 4, loss = 0.70916568\n",
      "Iteration 5, loss = 0.56553859\n",
      "Iteration 6, loss = 0.46975028\n",
      "Iteration 7, loss = 0.39615480\n",
      "Iteration 8, loss = 0.34226399\n",
      "Iteration 9, loss = 0.29786599\n",
      "Iteration 10, loss = 0.24980213\n",
      "Iteration 11, loss = 0.22032043\n",
      "Iteration 12, loss = 0.18867735\n",
      "Iteration 13, loss = 0.16934855\n",
      "Iteration 14, loss = 0.14608341\n",
      "Iteration 15, loss = 0.12224052\n",
      "Iteration 16, loss = 0.13051899\n",
      "Iteration 17, loss = 0.10974488\n",
      "Iteration 18, loss = 0.11499432\n",
      "Iteration 19, loss = 0.11206833\n",
      "Iteration 20, loss = 0.09796820\n",
      "Iteration 21, loss = 0.10882981\n",
      "Iteration 22, loss = 0.09985240\n",
      "Iteration 23, loss = 0.09300208\n",
      "Iteration 24, loss = 0.09790339\n",
      "Iteration 25, loss = 0.08750903\n",
      "Iteration 26, loss = 0.07477768\n",
      "Iteration 27, loss = 0.09086325\n",
      "Iteration 28, loss = 0.09536132\n",
      "Iteration 29, loss = 0.07677910\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.32924403\n",
      "Iteration 2, loss = 1.29767983\n",
      "Iteration 3, loss = 0.90312888\n",
      "Iteration 4, loss = 0.69043657\n",
      "Iteration 5, loss = 0.58177370\n",
      "Iteration 6, loss = 0.49415058\n",
      "Iteration 7, loss = 0.42665495\n",
      "Iteration 8, loss = 0.37692857\n",
      "Iteration 9, loss = 0.32256328\n",
      "Iteration 10, loss = 0.27894355\n",
      "Iteration 11, loss = 0.23516360\n",
      "Iteration 12, loss = 0.20620267\n",
      "Iteration 13, loss = 0.18190042\n",
      "Iteration 14, loss = 0.16769251\n",
      "Iteration 15, loss = 0.14770599\n",
      "Iteration 16, loss = 0.15224862\n",
      "Iteration 17, loss = 0.12295493\n",
      "Iteration 18, loss = 0.11202979\n",
      "Iteration 19, loss = 0.12167574\n",
      "Iteration 20, loss = 0.09489472\n",
      "Iteration 21, loss = 0.09491939\n",
      "Iteration 22, loss = 0.08188371\n",
      "Iteration 23, loss = 0.10272192\n",
      "Iteration 24, loss = 0.09861583\n",
      "Iteration 25, loss = 0.09707781\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.22154319\n",
      "Iteration 2, loss = 1.39614167\n",
      "Iteration 3, loss = 0.92332598\n",
      "Iteration 4, loss = 0.72394102\n",
      "Iteration 5, loss = 0.62694502\n",
      "Iteration 6, loss = 0.54236866\n",
      "Iteration 7, loss = 0.47034608\n",
      "Iteration 8, loss = 0.41920172\n",
      "Iteration 9, loss = 0.37481416\n",
      "Iteration 10, loss = 0.32888924\n",
      "Iteration 11, loss = 0.28835675\n",
      "Iteration 12, loss = 0.24236982\n",
      "Iteration 13, loss = 0.21886904\n",
      "Iteration 14, loss = 0.19019750\n",
      "Iteration 15, loss = 0.15487758\n",
      "Iteration 16, loss = 0.14527735\n",
      "Iteration 17, loss = 0.13869390\n",
      "Iteration 18, loss = 0.16116208\n",
      "Iteration 19, loss = 0.20134928\n",
      "Iteration 20, loss = 0.21487057\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39520936\n",
      "Iteration 2, loss = 1.65894191\n",
      "Iteration 3, loss = 1.17583220\n",
      "Iteration 4, loss = 0.88862547\n",
      "Iteration 5, loss = 0.73578722\n",
      "Iteration 6, loss = 0.63413268\n",
      "Iteration 7, loss = 0.55342351\n",
      "Iteration 8, loss = 0.50647918\n",
      "Iteration 9, loss = 0.48289177\n",
      "Iteration 10, loss = 0.42935993\n",
      "Iteration 11, loss = 0.39195638\n",
      "Iteration 12, loss = 0.34054704\n",
      "Iteration 13, loss = 0.30069730\n",
      "Iteration 14, loss = 0.27793631\n",
      "Iteration 15, loss = 0.26033615\n",
      "Iteration 16, loss = 0.27240650\n",
      "Iteration 17, loss = 0.22153220\n",
      "Iteration 18, loss = 0.19799906\n",
      "Iteration 19, loss = 0.22254360\n",
      "Iteration 20, loss = 0.19262536\n",
      "Iteration 21, loss = 0.16642479\n",
      "Iteration 22, loss = 0.14924888\n",
      "Iteration 23, loss = 0.15303996\n",
      "Iteration 24, loss = 0.15168865\n",
      "Iteration 25, loss = 0.11810074\n",
      "Iteration 26, loss = 0.10602040\n",
      "Iteration 27, loss = 0.10559034\n",
      "Iteration 28, loss = 0.09090759\n",
      "Iteration 29, loss = 0.09227500\n",
      "Iteration 30, loss = 0.07132521\n",
      "Iteration 31, loss = 0.08849219\n",
      "Iteration 32, loss = 0.07718825\n",
      "Iteration 33, loss = 0.06932414\n",
      "Iteration 34, loss = 0.07048719\n",
      "Iteration 35, loss = 0.11984764\n",
      "Iteration 36, loss = 0.27190721\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.35280068\n",
      "Iteration 2, loss = 1.51396481\n",
      "Iteration 3, loss = 1.04775399\n",
      "Iteration 4, loss = 0.85856085\n",
      "Iteration 5, loss = 0.72302966\n",
      "Iteration 6, loss = 0.64593378\n",
      "Iteration 7, loss = 0.56327799\n",
      "Iteration 8, loss = 0.52203230\n",
      "Iteration 9, loss = 0.47992416\n",
      "Iteration 10, loss = 0.43433626\n",
      "Iteration 11, loss = 0.38181135\n",
      "Iteration 12, loss = 0.36144600\n",
      "Iteration 13, loss = 0.36295667\n",
      "Iteration 14, loss = 0.32740314\n",
      "Iteration 15, loss = 0.34757405\n",
      "Iteration 16, loss = 0.31504656\n",
      "Iteration 17, loss = 0.33194154\n",
      "Iteration 18, loss = 0.29322420\n",
      "Iteration 19, loss = 0.27017172\n",
      "Iteration 20, loss = 0.27593563\n",
      "Iteration 21, loss = 0.24590521\n",
      "Iteration 22, loss = 0.22702742\n",
      "Iteration 23, loss = 0.20554947\n",
      "Iteration 24, loss = 0.19521027\n",
      "Iteration 25, loss = 0.18010390\n",
      "Iteration 26, loss = 0.17279468\n",
      "Iteration 27, loss = 0.16624751\n",
      "Iteration 28, loss = 0.15218984\n",
      "Iteration 29, loss = 0.13153106\n",
      "Iteration 30, loss = 0.11863077\n",
      "Iteration 31, loss = 0.11498634\n",
      "Iteration 32, loss = 0.09366294\n",
      "Iteration 33, loss = 0.09250325\n",
      "Iteration 34, loss = 0.10318085\n",
      "Iteration 35, loss = 0.10939284\n",
      "Iteration 36, loss = 0.12030502\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38160539\n",
      "Iteration 2, loss = 1.45214593\n",
      "Iteration 3, loss = 1.09231709\n",
      "Iteration 4, loss = 0.89085833\n",
      "Iteration 5, loss = 0.71886152\n",
      "Iteration 6, loss = 0.64891287\n",
      "Iteration 7, loss = 0.58378552\n",
      "Iteration 8, loss = 0.53288669\n",
      "Iteration 9, loss = 0.50917325\n",
      "Iteration 10, loss = 0.46150809\n",
      "Iteration 11, loss = 0.42355525\n",
      "Iteration 12, loss = 0.40370627\n",
      "Iteration 13, loss = 0.42381153\n",
      "Iteration 14, loss = 0.42720369\n",
      "Iteration 15, loss = 0.44048627\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.47739458\n",
      "Iteration 2, loss = 1.83549127\n",
      "Iteration 3, loss = 1.42263536\n",
      "Iteration 4, loss = 1.10642598\n",
      "Iteration 5, loss = 0.98112302\n",
      "Iteration 6, loss = 0.92292468\n",
      "Iteration 7, loss = 0.85132799\n",
      "Iteration 8, loss = 0.80745965\n",
      "Iteration 9, loss = 0.77052315\n",
      "Iteration 10, loss = 0.73041053\n",
      "Iteration 11, loss = 0.69151341\n",
      "Iteration 12, loss = 0.63067266\n",
      "Iteration 13, loss = 0.56341801\n",
      "Iteration 14, loss = 0.52599180\n",
      "Iteration 15, loss = 0.50509239\n",
      "Iteration 16, loss = 0.47547137\n",
      "Iteration 17, loss = 0.46117887\n",
      "Iteration 18, loss = 0.45618355\n",
      "Iteration 19, loss = 0.44450522\n",
      "Iteration 20, loss = 0.45461416\n",
      "Iteration 21, loss = 0.44348343\n",
      "Iteration 22, loss = 0.39298474\n",
      "Iteration 23, loss = 0.37203983\n",
      "Iteration 24, loss = 0.38142722\n",
      "Iteration 25, loss = 0.37089223\n",
      "Iteration 26, loss = 0.37482211\n",
      "Iteration 27, loss = 0.36877460\n",
      "Iteration 28, loss = 0.35583666\n",
      "Iteration 29, loss = 0.35483756\n",
      "Iteration 30, loss = 0.32213509\n",
      "Iteration 31, loss = 0.33462796\n",
      "Iteration 32, loss = 0.30704936\n",
      "Iteration 33, loss = 0.41500345\n",
      "Iteration 34, loss = 0.28065040\n",
      "Iteration 35, loss = 0.27611333\n",
      "Iteration 36, loss = 0.28099529\n",
      "Iteration 37, loss = 0.35423393\n",
      "Iteration 38, loss = 0.29057662\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.52709240\n",
      "Iteration 2, loss = 1.83338902\n",
      "Iteration 3, loss = 1.36309188\n",
      "Iteration 4, loss = 1.19952325\n",
      "Iteration 5, loss = 1.08280261\n",
      "Iteration 6, loss = 0.94255831\n",
      "Iteration 7, loss = 0.84623416\n",
      "Iteration 8, loss = 0.77274834\n",
      "Iteration 9, loss = 0.69525814\n",
      "Iteration 10, loss = 0.65128238\n",
      "Iteration 11, loss = 0.63385218\n",
      "Iteration 12, loss = 0.58476819\n",
      "Iteration 13, loss = 0.56500844\n",
      "Iteration 14, loss = 0.54774194\n",
      "Iteration 15, loss = 0.54464632\n",
      "Iteration 16, loss = 0.50189388\n",
      "Iteration 17, loss = 0.46956437\n",
      "Iteration 18, loss = 0.43353123\n",
      "Iteration 19, loss = 0.42108982\n",
      "Iteration 20, loss = 0.40118729\n",
      "Iteration 21, loss = 0.38953994\n",
      "Iteration 22, loss = 0.37950300\n",
      "Iteration 23, loss = 0.36436254\n",
      "Iteration 24, loss = 0.34761905\n",
      "Iteration 25, loss = 0.34091307\n",
      "Iteration 26, loss = 1.84289421\n",
      "Iteration 27, loss = 0.80670681\n",
      "Iteration 28, loss = 0.69474501\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.62456778\n",
      "Iteration 2, loss = 1.89406208\n",
      "Iteration 3, loss = 1.55306542\n",
      "Iteration 4, loss = 1.42226174\n",
      "Iteration 5, loss = 1.32753076\n",
      "Iteration 6, loss = 1.27198379\n",
      "Iteration 7, loss = 1.24267692\n",
      "Iteration 8, loss = 1.24998349\n",
      "Iteration 9, loss = 1.16166095\n",
      "Iteration 10, loss = 1.13649288\n",
      "Iteration 11, loss = 1.10672891\n",
      "Iteration 12, loss = 1.01229797\n",
      "Iteration 13, loss = 0.95114581\n",
      "Iteration 14, loss = 0.86080110\n",
      "Iteration 15, loss = 0.82237908\n",
      "Iteration 16, loss = 0.79824739\n",
      "Iteration 17, loss = 1.79357652\n",
      "Iteration 18, loss = 0.97510120\n",
      "Iteration 19, loss = 0.94432359\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.70189803\n",
      "Iteration 2, loss = 2.01847714\n",
      "Iteration 3, loss = 1.67693440\n",
      "Iteration 4, loss = 1.62861672\n",
      "Iteration 5, loss = 1.61648434\n",
      "Iteration 6, loss = 1.57279120\n",
      "Iteration 7, loss = 1.52317066\n",
      "Iteration 8, loss = 1.40903743\n",
      "Iteration 9, loss = 1.31559559\n",
      "Iteration 10, loss = 1.28483878\n",
      "Iteration 11, loss = 1.25853026\n",
      "Iteration 12, loss = 1.19768758\n",
      "Iteration 13, loss = 1.13990211\n",
      "Iteration 14, loss = 1.09321459\n",
      "Iteration 15, loss = 1.04283001\n",
      "Iteration 16, loss = 0.97883310\n",
      "Iteration 17, loss = 0.92501918\n",
      "Iteration 18, loss = 0.89213511\n",
      "Iteration 19, loss = 0.90995485\n",
      "Iteration 20, loss = 0.99884480\n",
      "Iteration 21, loss = 0.94409815\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.67479967\n",
      "Iteration 2, loss = 2.08928169\n",
      "Iteration 3, loss = 1.68045385\n",
      "Iteration 4, loss = 1.70794161\n",
      "Iteration 5, loss = 1.64337833\n",
      "Iteration 6, loss = 1.61463208\n",
      "Iteration 7, loss = 1.53250839\n",
      "Iteration 8, loss = 1.41576502\n",
      "Iteration 9, loss = 1.31489552\n",
      "Iteration 10, loss = 1.29277403\n",
      "Iteration 11, loss = 1.24446423\n",
      "Iteration 12, loss = 1.21969175\n",
      "Iteration 13, loss = 1.15769952\n",
      "Iteration 14, loss = 1.11188185\n",
      "Iteration 15, loss = 1.04128673\n",
      "Iteration 16, loss = 0.98466438\n",
      "Iteration 17, loss = 0.92885883\n",
      "Iteration 18, loss = 0.90728107\n",
      "Iteration 19, loss = 0.85673629\n",
      "Iteration 20, loss = 0.85778664\n",
      "Iteration 21, loss = 0.78956757\n",
      "Iteration 22, loss = 0.80730478\n",
      "Iteration 23, loss = 0.76926826\n",
      "Iteration 24, loss = 0.73885886\n",
      "Iteration 25, loss = 0.70715714\n",
      "Iteration 26, loss = 0.69855153\n",
      "Iteration 27, loss = 0.67395434\n",
      "Iteration 28, loss = 0.65435316\n",
      "Iteration 29, loss = 0.70592678\n",
      "Iteration 30, loss = 0.74717175\n",
      "Iteration 31, loss = 0.79969389\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.51763023\n",
      "Iteration 2, loss = 1.85521030\n",
      "Iteration 3, loss = 1.71522978\n",
      "Iteration 4, loss = 1.64090394\n",
      "Iteration 5, loss = 1.61026496\n",
      "Iteration 6, loss = 1.58327588\n",
      "Iteration 7, loss = 1.54155859\n",
      "Iteration 8, loss = 1.44704125\n",
      "Iteration 9, loss = 1.35929442\n",
      "Iteration 10, loss = 1.28418592\n",
      "Iteration 11, loss = 1.25359813\n",
      "Iteration 12, loss = 1.20254575\n",
      "Iteration 13, loss = 1.19824668\n",
      "Iteration 14, loss = 1.13795455\n",
      "Iteration 15, loss = 1.06749500\n",
      "Iteration 16, loss = 0.98899599\n",
      "Iteration 17, loss = 0.93859201\n",
      "Iteration 18, loss = 0.94866714\n",
      "Iteration 19, loss = 0.88031943\n",
      "Iteration 20, loss = 0.83885562\n",
      "Iteration 21, loss = 0.81736368\n",
      "Iteration 22, loss = 0.75755963\n",
      "Iteration 23, loss = 0.74585462\n",
      "Iteration 24, loss = 0.70467462\n",
      "Iteration 25, loss = 0.64782192\n",
      "Iteration 26, loss = 0.62807138\n",
      "Iteration 27, loss = 0.58370710\n",
      "Iteration 28, loss = 0.55611642\n",
      "Iteration 29, loss = 0.51528876\n",
      "Iteration 30, loss = 0.48377291\n",
      "Iteration 31, loss = 1.01247251\n",
      "Iteration 32, loss = 0.76879546\n",
      "Iteration 33, loss = 0.74723324\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.57596775\n",
      "Iteration 2, loss = 1.95197076\n",
      "Iteration 3, loss = 1.71986502\n",
      "Iteration 4, loss = 1.62882081\n",
      "Iteration 5, loss = 1.63904454\n",
      "Iteration 6, loss = 1.60946656\n",
      "Iteration 7, loss = 1.61082688\n",
      "Iteration 8, loss = 1.60582408\n",
      "Iteration 9, loss = 1.59949331\n",
      "Iteration 10, loss = 1.59901637\n",
      "Iteration 11, loss = 1.59707755\n",
      "Iteration 12, loss = 1.60177558\n",
      "Iteration 13, loss = 1.60740819\n",
      "Iteration 14, loss = 1.59969698\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "#Vary number of layers\n",
    "\n",
    "N_nodes = 50\n",
    "N_layers = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 14, 18, 22, 26, 30])\n",
    "accuracy = []\n",
    "\n",
    "for nlayers in N_layers:\n",
    "\n",
    "    layers = np.multiply(N_nodes, np.ones(nlayers)) \n",
    "    scaler = StandardScaler(with_mean = False)\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=layers, max_iter=max_iter, alpha=alpha,\n",
    "                        solver=solver, verbose=10, tol=tol, random_state=1,\n",
    "                        learning_rate_init=rate_init)\n",
    "    mlp.fit(X_train_scaled, t_train)\n",
    "\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    preds = mlp.predict(X_test_scaled)\n",
    "\n",
    "    #test predictions\n",
    "    num_correct = np.sum(t_ignore == preds);\n",
    "    accuracy = np.append(accuracy,num_correct/float(t_ignore.size))\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(N_layers, accuracy, linestyle='--', marker='o', color='b')\n",
    "#plt.title('N_nodes = '+ str(N_nodes)+'_N_layers = ' + str(N_layers))\n",
    "plt.xlabel('Number of layers')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig('Plots/'+_N_nodes_'+ str(N_nodes)+'_N_layers' + str(N_layers)+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Try decition tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train,t_train)\n",
    "preds = clf.predict(X_test)\n",
    "accuracy = np.sum(t_ignore == preds)/float(t_ignore.size)\n",
    "print(accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.888349514563\n"
     ]
    }
   ],
   "source": [
    "#Try random forest\n",
    "X_subset_train, X_subset_test, t_subset_train, t_subset_test = train_test_split(X_train, t_train, test_size=0.2, random_state=1)\n",
    "scaler = StandardScaler(with_mean = False)\n",
    "scaler.fit(X_subset_train)\n",
    "X_train_scaled = scaler.transform(X_subset_train)\n",
    "clf = RandomForestClassifier(n_jobs=-1, n_estimators = 10000)# class_weight='balanced')\n",
    "#y, _ = pd.factorize(train['species'])\n",
    "clf.fit(X_train_scaled, t_subset_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_subset_test)\n",
    "preds = clf.predict(X_test_scaled)\n",
    "accuracy = np.sum(t_subset_test == preds)/float(t_subset_test.size)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.65149104\n",
      "Validation score: 0.783172\n",
      "Iteration 2, loss = 0.76746184\n",
      "Validation score: 0.796117\n",
      "Iteration 3, loss = 0.56940306\n",
      "Validation score: 0.841424\n",
      "Iteration 4, loss = 0.45678843\n",
      "Validation score: 0.857605\n",
      "Iteration 5, loss = 0.39835289\n",
      "Validation score: 0.870550\n",
      "Iteration 6, loss = 0.35105035\n",
      "Validation score: 0.877023\n",
      "Iteration 7, loss = 0.31999594\n",
      "Validation score: 0.880259\n",
      "Iteration 8, loss = 0.28653051\n",
      "Validation score: 0.877023\n",
      "Iteration 9, loss = 0.27527829\n",
      "Validation score: 0.883495\n",
      "Iteration 10, loss = 0.25088694\n",
      "Validation score: 0.899676\n",
      "Iteration 11, loss = 0.24069678\n",
      "Validation score: 0.899676\n",
      "Iteration 12, loss = 0.23830228\n",
      "Validation score: 0.889968\n",
      "Iteration 13, loss = 0.24147302\n",
      "Validation score: 0.899676\n",
      "Validation score did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "writing predictions...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "#predict using MLP\n",
    "\n",
    "train_dir = \"train\"\n",
    "test_dir = \"test\"\n",
    "outputfile = \"MLP_preds.csv\"  # feel free to change this or take it as an argument\n",
    "\n",
    "# TODO put the names of the feature functions you've defined above in this list\n",
    "ffs = [first_last_system_call_feats, system_call_count_feats]\n",
    "\n",
    "# extract features\n",
    "print \"extracting training features...\"\n",
    "X_train,global_feat_dict,t_train,train_ids = extract_feats(ffs, train_dir)\n",
    "print \"done extracting training features\"\n",
    "print\n",
    "\n",
    "\n",
    "print \"extracting test features...\"\n",
    "X_test,_,t_ignore,test_ids = extract_feats(ffs, test_dir, global_feat_dict=global_feat_dict)\n",
    "print \"done extracting test features\"\n",
    "print\n",
    "\n",
    "solver = 'adam'\n",
    "#momentum = 0.9 #only is sgd\n",
    "#nesterovs_momentum = True # only if sgd\n",
    "\n",
    "max_iter =100\n",
    "batch_size = 200\n",
    "solver = 'adam'\n",
    "max_iter = 500\n",
    "\n",
    "\n",
    "alpha = 1e-4\n",
    "tol = 1e-12\n",
    "rate_init = 0.01\n",
    "learning_rate = 'adaptive' #only matters for sgd\n",
    "activation = 'relu'\n",
    "early_stopping = True\n",
    "#validation_fraction = 0.1\n",
    "#warm_start = False\n",
    "\n",
    "N_nodes = 25\n",
    "N_layers = 3\n",
    "\n",
    "layers = np.multiply(N_nodes, np.ones(N_layers)) \n",
    "scaler = StandardScaler(with_mean = False)\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=layers, max_iter=max_iter, alpha=alpha,\n",
    "                        solver=solver, verbose=10, tol=tol, random_state=1,\n",
    "                        learning_rate_init=rate_init, activation = activation,\n",
    "                   learning_rate = learning_rate, early_stopping=early_stopping,\n",
    "                   #validation_fraction = validation_fraction, warm_start = warm_start, batch_size = batch_size, \n",
    "                   #momentum = momentum, nesterovs_momentum=nesterovs_momentum\n",
    "                   )\n",
    "mlp.fit(X_train_scaled, t_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "preds = mlp.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "print \"writing predictions...\"\n",
    "util.write_predictions(preds, test_ids, outputfile)\n",
    "print \"done!\"\n",
    "#num_correct = np.sum(t_ignore == preds);\n",
    "#accuracy = num_correct/float(t_ignore.size)\n",
    "#print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]0.864077669903\n"
     ]
    }
   ],
   "source": [
    "#Try SVM\n",
    "\n",
    "\n",
    "X_subset_train, X_subset_test, t_subset_train, t_subset_test = train_test_split(X_train, t_train, test_size=0.2, random_state=1)\n",
    "\n",
    "kernel = 'rbf'\n",
    "tol = 1e-3\n",
    "#class_weight = 'balanced'\n",
    "\n",
    "scaler = StandardScaler(with_mean = False)\n",
    "scaler.fit(X_subset_train)\n",
    "X_train_scaled = scaler.transform(X_subset_train)\n",
    "clf = svm.SVC(verbose = True, kernel = kernel, tol = tol)\n",
    "clf.fit(X_train_scaled, t_subset_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_subset_test)\n",
    "preds = clf.predict(X_test_scaled)\n",
    "\n",
    "#test predictions\n",
    "num_correct = np.sum(t_subset_test == preds);\n",
    "accuracy = num_correct/float(t_subset_test.size)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]writing predictions...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "#Predict using SVM\n",
    "outputfile = 'SVM_preds.csv'\n",
    "scaler = StandardScaler(with_mean = False)\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "clf = svm.SVC(verbose = True, kernel = kernel, tol = tol)\n",
    "clf.fit(X_train_scaled, t_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "preds = clf.predict(X_test_scaled)\n",
    "print \"writing predictions...\"\n",
    "util.write_predictions(preds, test_ids, outputfile)\n",
    "print \"done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
